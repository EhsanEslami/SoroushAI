{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import stat\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts.chat import (\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document, StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from colorama import Fore\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "LANGUAGE_MODEL = \"gpt-4 turbo\"\n",
    "llm = ChatOpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ارزیاب بازیابی : ارزیاب ارتباط ####\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"امتیاز باینری برای ارزیابی ارتباط اسناد بازیابی‌شده.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"اسناد نسبت به پرسش کاربر مرتبط هستند: 'بله' یا 'خیر'\")\n",
    "\n",
    "    def get_score(self) -> str:\n",
    "        \"\"\"امتیاز باینری را به صورت رشته برمی‌گرداند.\"\"\"\n",
    "        return self.binary_score\n",
    "\n",
    "\n",
    "def get_score(self) -> str:\n",
    "    \"\"\"امتیاز باینری را به صورت رشته برمی‌گرداند.\"\"\"\n",
    "    return self.binary_score\n",
    "\n",
    "# استفاده از LLM با فراخوانی تابع ساختارمند\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt \n",
    "\n",
    "system_template = \"\"\"تو یک ارزیاب هستی که ارتباط اسناد بازیابی شده زیر:/\n",
    "{documents}/\n",
    "با پرسش کاربر:/\n",
    "{question}/\n",
    "را تعیین می کنید. اگر سند شامل کلمه های کلیدی با معانی مرتبط با پرسش باشد آنرا به عنوان مرتبط علامتگداری کرده و پاسخ:\n",
    "بله/\n",
    "و اگر نه پاسخ:\n",
    "خیر/\n",
    "را انتخاب کنید./\n",
    "سعی کن در پاسخ خود سخت گیر باشی و اگر اطمینان نداری که سند مرتبط است پاسخ خیر را انتخاب کنی./\"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    input_variables=[\"documents\", \"question\"],\n",
    "    template=\"{question}\",\n",
    ")\n",
    "grader_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "def load_documents():\n",
    "    \"\"\"\n",
    "    Load all text files from a directory, split them into chunks,\n",
    "    and add metadata with 'doc_id' and 'chunk_index' for each chunk.\n",
    "    \"\"\"\n",
    "    loader = DirectoryLoader(\"./output_text/\", glob=\"*.txt\")  # Load all .txt files\n",
    "    raw_documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=0,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    for raw_doc in raw_documents:\n",
    "        # Get a document identifier. Here we use the 'source' metadata if available.\n",
    "        doc_id = raw_doc.metadata.get(\"source\", \"unknown\")\n",
    "        chunks = text_splitter.split_text(raw_doc.page_content)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            new_doc = Document(page_content=chunk, metadata={\"doc_id\": doc_id, \"chunk_index\": idx})\n",
    "            all_chunks.append(new_doc)\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "def load_embeddings(documents, user_query):\n",
    "    \"\"\"\n",
    "    Create or load a Chroma vector store from a set of documents.\n",
    "    \"\"\"\n",
    "    persist_directory = './chroma_cache'  # Directory to store embeddings\n",
    "    embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "    # Ensure the directory exists and has write permissions\n",
    "    if not os.path.exists(persist_directory):\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "    else:\n",
    "        if not os.access(persist_directory, os.W_OK):\n",
    "            print(f\"Error: No write access to {persist_directory}. Fixing permissions...\")\n",
    "            try:\n",
    "                os.chmod(persist_directory, stat.S_IWUSR | stat.S_IRUSR | stat.S_IXUSR)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to change directory permissions: {e}\")\n",
    "                return None\n",
    "\n",
    "    try:\n",
    "        # Load or create Chroma vector store\n",
    "        if not os.listdir(persist_directory):  # Empty directory means no existing DB\n",
    "            print(\"Initializing new ChromaDB instance...\")\n",
    "            db = Chroma.from_documents(documents, embedding_model, persist_directory=persist_directory)\n",
    "            db.persist()\n",
    "        else:\n",
    "            print(\"Loading existing ChromaDB instance...\")\n",
    "            db = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
    "\n",
    "        # For debugging: perform a similarity search with score and print the top result\n",
    "        docs_with_scores = db.similarity_search_with_score(user_query, k=1)\n",
    "        if docs_with_scores:\n",
    "            top_doc, score = docs_with_scores[0]\n",
    "            print(\"\\nRetrieved Document (for debugging):\\n\")\n",
    "            print(format_docs([top_doc]))\n",
    "            print(\"\\nSimilarity Score:\", score)\n",
    "        else:\n",
    "            print(\"No documents retrieved for the query.\")\n",
    "\n",
    "        return db.as_retriever()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while loading ChromaDB: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_chunk(retriever , query):\n",
    "\n",
    "    retrieved_chunks = retriever.get_relevant_documents(query)\n",
    "    if not retrieved_chunks:\n",
    "        return \"No relevant document found.\"\n",
    "\n",
    "    # Retrieve the top (most relevant) chunk and extract doc_id\n",
    "    top_chunk = retrieved_chunks[0]\n",
    "\n",
    "    return top_chunk.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global documents for dynamic neighbor retrieval\n",
    "documents = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing ChromaDB instance...\n",
      "\n",
      "Retrieved Document (for debugging):\n",
      "\n",
      "از بیالفتی است، دل که دل بردید کی ماند ترش، بلبلی گل دید، کی ماند خمشماهی بریانز آسیب خرزر زنده شد،\n",
      "\n",
      "Similarity Score: 0.24009285867214203\n"
     ]
    }
   ],
   "source": [
    "query_text_1 = \"\"\"تفسیر بیت زیر چیست: /\"\n",
    "\"یاد من کن پیش تخت آن عزیز /\n",
    "تا مرا هم واخرد زین حبس نیز\"\"\"\n",
    "\n",
    "query_text_2 = \"\"\"تفسیر بیت زیر چیست: /\n",
    "\n",
    "ای حیات دل حسام‌الدین بسی/\n",
    "میل می‌جوشد به قسم سادسی\"\"\"\n",
    "\n",
    "query_text_3 = \"\"\"تفسیر بیت زیر چیست: /\n",
    "گشت از جذب چو تو علامه‌ای/\n",
    "در جهان گردان حسامی نامه‌ای\"\"\"\n",
    "\n",
    "query_text_4 = \"\"\"تفسیر بیت زیر چیست: /\n",
    "\"عارفی پرسید از آن پیر کشیش/\n",
    "که توی خواجه مسن‌تر یا که ریش\n",
    "\"\"\"\n",
    "\n",
    "query_text_5 = \"\"\"تفسیر بیت زیر چیست: /\n",
    "خواجه‌ای را بود هندو بنده‌ای/\n",
    "پروریده کرده او را زنده‌ای\"\"\"\n",
    "\n",
    "query_text = query_text_2\n",
    "\n",
    "retriever = load_embeddings(documents, query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "از بیالفتی است، دل که دل بردید کی ماند ترش، بلبلی گل دید، کی ماند خمشماهی بریانز آسیب خرزر زنده شد،\n"
     ]
    }
   ],
   "source": [
    "context = top_chunk(retriever, query_text)\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_retrieve_docs(query, context):\n",
    "\n",
    "    retrieval_grader = grader_prompt | structured_llm_grader | get_score\n",
    "    binary_score = retrieval_grader.invoke({\"question\": query, \"documents\": context})\n",
    "    \n",
    "    return binary_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary score: خیر\n"
     ]
    }
   ],
   "source": [
    "binary_score = assess_retrieve_docs(query_text, context)\n",
    "print(\"binary score:\", binary_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template: str = \"\"\"/\n",
    "    فرض کن تو یک مفسر مثنوی هستی. جواب سوال زیر را بده: /\n",
    "      {question} /\n",
    "   از محتوای زیر برای پیدا کردن مفاهیم و زمینه مربوطه استفاده کن. محتوای زیر از جلسات تفسیر مثنوی معنوی عبدالکریم سروش گرفته شده است./\n",
    "      {context} /\n",
    "      این محتوا از جلسه شماره زیر گرفته شده است:/\n",
    "      {doc_name}/\n",
    "       . سعی کن از این محتوا برای فهمیدن داستان و تاریخ مربوطه و ابیات مجاور بیت مورد سوال استفاده کنی./\n",
    "       در پاسخی که میدهی سعی کن به زمینه داستانی و تاریخی و اشعار مجاور شعر مورد سوال در مثنوی اشاره کنی./\n",
    "       در پاسخ خود به شماره جلسه ای که در مورد این شعر صحبت شده نیز اشاره کن./\n",
    "       در پاسخ خود به تمام جزییاتی که از متن جلسه دریافت می کنی و مرتبط با سوال مطرح شده است اشاره کن.\n",
    "    \"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"{question}\",\n",
    ")\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")\n",
    "\n",
    "### Question Re-writer - Knowledge Refinement ####\n",
    "# Prompt \n",
    "prompt_template = \"\"\"با در نظر گرفتن سوال زیر:/\n",
    "{question},/\n",
    " دقت کن که سوال را به گونه ای بازنویسی کنی که مدل زبانی بتواند بهترین پاسخ را ارائه دهد./\n",
    " به سوال پاسخ نده. فقط سوال را بازنویسی کن./\"\"\"\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(prompt_template)\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"{question}\",\n",
    ")\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_prompt, human_prompt]\n",
    ")\n",
    "\n",
    "### Web Search Tool - Knowledge Searching ####\n",
    "web_search_tool = TavilySearchResults(k=3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(query):\n",
    "    question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "    return question_rewriter.invoke({\"question\": query})\n",
    "\n",
    "def search_web(query):\n",
    "    docs = web_search_tool.invoke({\"query\": query})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    return Document(page_content=web_results)\n",
    "    \n",
    "def generate_response(retriever, query):\n",
    "    retrieved_chunks = retriever.get_relevant_documents(query)\n",
    "    if not retrieved_chunks:\n",
    "        return \"No relevant document found.\"\n",
    "\n",
    "    # Retrieve the top (most relevant) chunk and extract doc_id\n",
    "    top_chunk = retrieved_chunks[0]\n",
    "    doc_id = top_chunk.metadata.get(\"doc_id\")\n",
    "    chunk_index = top_chunk.metadata.get(\"chunk_index\")\n",
    "\n",
    "    # Find all chunks from the same document (using the global 'documents' variable)\n",
    "    same_doc_chunks = [doc for doc in documents if doc.metadata.get(\"doc_id\") == doc_id]\n",
    "    same_doc_chunks = sorted(same_doc_chunks, key=lambda d: d.metadata.get(\"chunk_index\", 0))\n",
    "\n",
    "    # Define a window: e.g. 15 chunks before and after the top chunk\n",
    "    start = max(0, chunk_index - 15)\n",
    "    end = min(len(same_doc_chunks), chunk_index + 15)\n",
    "    aggregated_context = \"\\n\\n\".join([doc.page_content for doc in same_doc_chunks[start:end]])\n",
    "\n",
    "    # Build the chain and invoke it with the additional 'doc_name' variable\n",
    "    chain = chat_prompt_template | llm | StrOutputParser()\n",
    "    input_vars = {\"context\": aggregated_context, \"question\": query, \"doc_name\": doc_id}\n",
    "    return chain.invoke(input_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mRetrieval is not relevant. Searching the web...\u001b[39m\n",
      "\u001b[33mRetrieval, rewriting and optmize the query...\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "if binary_score == \"خیر\":\n",
    "        print(f\"{Fore.MAGENTA}Retrieval is not relevant. Searching the web...{Fore.RESET}\")\n",
    "        context = search_web(query_text) \n",
    "\n",
    "print(f\"{Fore.YELLOW}Retrieval, rewriting and optmize the query...{Fore.RESET}\")  \n",
    "optimized_query = rewrite_query(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='001 ای حیات دل! حُسامالدین! بســی مِیل میجوشد به قسـمِ سـادسی. وقتی که میگوید حیات دل یعنی ای حسام الدینی که بخشنده حیات معنوی به دلهای مرده\\nتفسیر مثنوی دفتر ۶ جلسهٔ ۱ بخش ۱ ۱ای حیات دل حسام\\u200cالدین بسی میل می\\u200cجوشد به قسم سادسی عموماً حُسام الدین ویا شمس کنایه ویا نمادی از منبع آگاهی\\nدیباچه دفتر ششم مثنوی معنوی مولو ی | دکلمه ، شرح و تفسیر ، دفتر ششم مثنوی معنوی از مولانا جلال الدین محمد بلخی درمرکز شعر و عرفان وب سایت\\nدر این کتاب ۴۲۴ داستان پی\\u200cدرپی به شیوهٔ تمثیل داستان سختی\\u200cهای انسان در راه رسیدن به خدا را بیان می\\u200cکند. هجده بیت نخست دفتر اول مثنوی معنوی به نی\\u200cنامه شهرت دارد و\\nمولانا یا روشنگری خود این مشکل بزرگ هدایت و راهیابی در طول زمانها را حل می کند. میفرماید که سیب هدایت و آگاهی، بوی خوشی به سوی تو خواهد آورد،اما کلید هدایت آن'\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بیت فوق الذکر از مثنوی مولوی است که به این معنا است که \"ای حیات، دل حسام‌الدین بسیار به سوی قسم سادسی میل و می‌جوشد\". این بیت به دو شخصیت مهم در مثنوی اشاره دارد. \n",
      "\n",
      "در تفسیر این بیت، می‌توان به داستان حسام‌الدین و قسم سادسی اشاره کرد. حسام‌الدین، شخصیتی است که در مثنوی مولوی به عنوان یک عاشق بی‌قرار و بی‌نیاز به عالم مادی نمایان شده است. او به دنبال روحانیت و معنویت است و به دنبال عشق الهی می‌گردد. قسم سادسی به دیگر یکی از شخصیت‌های مثنوی اشاره دارد که نماد عشق و معشوق الهی است. حسام‌الدین به سوی این قسم سادسی میل می‌جوشد، یعنی عشق و اشتیاق او به عشق الهی بسیار بزرگ و بی‌پایان است.\n",
      "\n",
      "در جلسه شماره ۶۱ دفتر ششم مثنوی، دکتر سروش در تفسیر این بیت به اشعار و مفاهیم مجاور اشاره کرده و روایت‌های مختلفی را برای شرح این بیت ارائه داده است. او به بررسی داستان حسام‌الدین و قسم سادسی پرداخته و تلاش کرده است تا عمق و غنای این بیت را برای شنوندگان روشن کند.\n",
      "\n",
      "در نهایت، این بیت نشان‌دهنده عشق و اشتیاق عمیق حسام‌الدین به معشوق الهی است و نشان‌دهنده تمایل او به سوی ارتباط و اتحاد با خداوند می‌باشد.\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(retriever, query_text)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soroush",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
