{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import stat\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts.chat import (\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document, StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from colorama import Fore\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGUAGE_MODEL = \"gpt-3.5-turbo-instruct\"\n",
    "LANGUAGE_MODEL = \"gpt-4-turbo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "template: str = \"\"\"/\n",
    "    فرض کن تو یک مفسر مثنوی هستی. جواب سوال زیر را بده: /\n",
    "      {question} /\n",
    "   از محتوای زیر برای پیدا کردن مفاهیم و زمینه مربوطه استفاده کن. محتوای زیر از جلسات تفسیر مثنوی معنوی عبدالکریم سروش گرفته شده است./\n",
    "      {context} /\n",
    "      این محتوا از جلسه شماره زیر گرفته شده است:/\n",
    "      {doc_name}/\n",
    "       . سعی کن از این محتوا برای فهمیدن داستان و تاریخ مربوطه و ابیات مجاور بیت مورد سوال استفاده کنی./\n",
    "       در پاسخی که میدهی سعی کن به زمینه داستانی و تاریخی و اشعار مجاور شعر مورد سوال در مثنوی اشاره کنی./\n",
    "       در پاسخ خود به شماره جلسه ای که در مورد این شعر صحبت شده نیز اشاره کن./\n",
    "       در پاسخ خود به تمام جزییاتی که از متن جلسه دریافت می کنی و مرتبط با سوال مطرح شده است اشاره کن.\n",
    "    \"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"{question}\",\n",
    ")\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "def load_documents():\n",
    "    \"\"\"\n",
    "    Load all text files from a directory, split them into chunks,\n",
    "    and add metadata with 'doc_id' and 'chunk_index' for each chunk.\n",
    "    \"\"\"\n",
    "    loader = DirectoryLoader(\"./output_text/\", glob=\"*.txt\")  # Load all .txt files\n",
    "    raw_documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=0,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    for raw_doc in raw_documents:\n",
    "        # Get a document identifier. Here we use the 'source' metadata if available.\n",
    "        doc_id = raw_doc.metadata.get(\"source\", \"unknown\")\n",
    "        chunks = text_splitter.split_text(raw_doc.page_content)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            new_doc = Document(page_content=chunk, metadata={\"doc_id\": doc_id, \"chunk_index\": idx})\n",
    "            all_chunks.append(new_doc)\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "def load_embeddings(documents, user_query):\n",
    "    \"\"\"\n",
    "    Create or load a Chroma vector store from a set of documents.\n",
    "    \"\"\"\n",
    "    persist_directory = './chroma_cache'  # Directory to store embeddings\n",
    "    embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "    # Ensure the directory exists and has write permissions\n",
    "    if not os.path.exists(persist_directory):\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "    else:\n",
    "        if not os.access(persist_directory, os.W_OK):\n",
    "            print(f\"Error: No write access to {persist_directory}. Fixing permissions...\")\n",
    "            try:\n",
    "                os.chmod(persist_directory, stat.S_IWUSR | stat.S_IRUSR | stat.S_IXUSR)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to change directory permissions: {e}\")\n",
    "                return None\n",
    "\n",
    "    try:\n",
    "        # Load or create Chroma vector store\n",
    "        if not os.listdir(persist_directory):  # Empty directory means no existing DB\n",
    "            print(\"Initializing new ChromaDB instance...\")\n",
    "            db = Chroma.from_documents(documents, embedding_model, persist_directory=persist_directory)\n",
    "            db.persist()\n",
    "        else:\n",
    "            print(\"Loading existing ChromaDB instance...\")\n",
    "            db = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
    "\n",
    "        # For debugging: perform a similarity search and print the top result\n",
    "        docs = db.similarity_search(user_query, k=1)\n",
    "        print(\"\\nRetrieved Document (for debugging):\\n\")\n",
    "        print(format_docs(docs))\n",
    "        return db.as_retriever()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while loading ChromaDB: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_response(retriever, query):\n",
    "    retrieved_chunks = retriever.get_relevant_documents(query)\n",
    "    if not retrieved_chunks:\n",
    "        return \"No relevant document found.\"\n",
    "\n",
    "    # Retrieve the top (most relevant) chunk and extract doc_id\n",
    "    top_chunk = retrieved_chunks[0]\n",
    "    doc_id = top_chunk.metadata.get(\"doc_id\")\n",
    "    chunk_index = top_chunk.metadata.get(\"chunk_index\")\n",
    "\n",
    "    # Find all chunks from the same document (using the global 'documents' variable)\n",
    "    same_doc_chunks = [doc for doc in documents if doc.metadata.get(\"doc_id\") == doc_id]\n",
    "    same_doc_chunks = sorted(same_doc_chunks, key=lambda d: d.metadata.get(\"chunk_index\", 0))\n",
    "\n",
    "    # Define a window: e.g. 15 chunks before and after the top chunk\n",
    "    start = max(0, chunk_index - 15)\n",
    "    end = min(len(same_doc_chunks), chunk_index + 15)\n",
    "    aggregated_context = \"\\n\\n\".join([doc.page_content for doc in same_doc_chunks[start:end]])\n",
    "\n",
    "    # Build the chain and invoke it with the additional 'doc_name' variable\n",
    "    chain = chat_prompt_template | model | StrOutputParser()\n",
    "    input_vars = {\"context\": aggregated_context, \"question\": query, \"doc_name\": doc_id}\n",
    "    return chain.invoke(input_vars)\n",
    "\n",
    "\n",
    "\n",
    "def query(query_text , documents_local):\n",
    "    \n",
    "    retriever_local = load_embeddings(documents_local, query_text)\n",
    "    response = generate_response(retriever_local, query_text)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global documents for dynamic neighbor retrieval\n",
    "documents = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"\"\"تفسیر بیت زیر چیست: /\"\n",
    "\"یاد من کن پیش تخت آن عزیز /\n",
    "تا مرا هم واخرد زین حبس نیز\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing ChromaDB instance...\n",
      "\n",
      "Retrieved Document (for debugging):\n",
      "\n",
      "یاد من کن پیش تخت آن عزیز. تا مرا هم واخرد زین حبس نیز. و مولوی اینجا را اشاره می کنیم. کی دهد\n"
     ]
    }
   ],
   "source": [
    "retriever = load_embeddings(documents, query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بیت زیر از مثنوی معنوی مولانا مولوی است که در آن یوسف در زندان به همراه دیگر زندانیان به شاهزاده اش اشاره می‌کند. یوسف، پس از آزادی از زندان و رسیدن به مقام بالاتر، خواستار یاد آوری خود نزد شاهزاده شده و از او درخواست می‌کند که او را نیز از زندان آزاد کند. \n",
      "\n",
      "این بیت نشان‌دهنده تواضع و فروتنی یوسف در مقام بالاتر است و او خواهان آزادی دیگر زندانیان نیز می‌شود، به‌طوری که از این حبس و زندانی بیرون آیند. این بیت از زمینه‌های تواضع، انسانیت، و اهمیت یادآوری کردن به یکدیگر و کمک به دیگران برای رسیدن به آزادی و سعادت بیان می‌کند.\n",
      "\n",
      "در جلسه شماره ۷۶ از تفسیر مثنوی معنوی عبدالکریم سروش، در مورد این بیت و معنای آن بحث شده است. این بیت در تفسیر اشعار مجاور و داستان مرتبط با آن در این جلسه به تفصیل بررسی شده است.\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(retriever, query_text)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soroush",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
